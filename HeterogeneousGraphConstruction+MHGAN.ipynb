{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0ff0a8fd-47ae-4217-b398-d7c3e9e19b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_type min/max: 0 2\n",
      "Epoch 001 | Train loss/acc/auc: 0.6856/0.5024/0.8347 | Val loss/acc/auc: 0.6872/0.5000/0.7827 | Test loss/acc/auc: 0.6854/0.5111/0.8474\n",
      "Epoch 010 | Train loss/acc/auc: 0.5050/0.8595/0.9908 | Val loss/acc/auc: 0.5141/0.8222/0.9951 | Test loss/acc/auc: 0.4977/0.8889/0.9911\n",
      "Epoch 020 | Train loss/acc/auc: 0.1648/0.9881/0.9974 | Val loss/acc/auc: 0.1834/0.9778/0.9970 | Test loss/acc/auc: 0.1800/0.9778/0.9951\n",
      "Epoch 030 | Train loss/acc/auc: 0.0654/0.9905/0.9991 | Val loss/acc/auc: 0.1364/0.9667/0.9970 | Test loss/acc/auc: 0.1041/0.9778/0.9960\n",
      "Epoch 040 | Train loss/acc/auc: 0.0485/0.9952/0.9991 | Val loss/acc/auc: 0.1313/0.9556/0.9960 | Test loss/acc/auc: 0.1098/0.9778/0.9936\n",
      "Epoch 050 | Train loss/acc/auc: 0.0348/0.9929/1.0000 | Val loss/acc/auc: 0.2118/0.9000/0.9931 | Test loss/acc/auc: 0.1274/0.9333/0.9901\n",
      "Epoch 060 | Train loss/acc/auc: 0.0430/0.9833/1.0000 | Val loss/acc/auc: 0.3883/0.8556/0.9906 | Test loss/acc/auc: 0.1883/0.9333/0.9881\n",
      "\n",
      "Final:\n",
      "Train: loss=0.0430, acc=0.9833, auc=1.0000\n",
      "Val  : loss=0.3883, acc=0.8556, auc=0.9906\n",
      "Test : loss=0.1883, acc=0.9333, auc=0.9881\n",
      "Saved: outputs/predictions_all_nodes.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) Reproducibility\n",
    "# ============================================================\n",
    "def set_seed(seed: int = 2):\n",
    "    \"\"\"Set seeds for python/numpy/torch to improve reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Feature config and edge-type IDs\n",
    "# ============================================================\n",
    "FEATURE_COLS = [\n",
    "    \"NDVI\", \"Disroad\", \"Driver\", \"Landuse\",\n",
    "    \"TWI\", \"SPI\", \"Relief\", \"Curvature\",\n",
    "    \"aspect\", \"Slope\", \"Lil\", \"Elevation\", \"rainfall\"\n",
    "]\n",
    "\n",
    "# Optional: type-aware feature subsets used for within-type similarity among positive nodes\n",
    "TYPE_FEATURES = {\n",
    "    1: [\"Driver\", \"TWI\", \"rainfall\", \"Elevation\", \"NDVI\", \"Slope\"],\n",
    "    2: [\"Disroad\", \"Landuse\", \"NDVI\"],\n",
    "    3: [\"Slope\", \"Relief\", \"TWI\"],\n",
    "    4: [\"TWI\", \"Relief\", \"Slope\", \"SPI\", \"NDVI\"],\n",
    "    5: [\"Driver\", \"NDVI\", \"Elevation\", \"TWI\", \"Slope\"],\n",
    "}\n",
    "\n",
    "# Edge types (must remain in [0..4])\n",
    "E_LL = 0   # landslide-landslide (within-type positives)\n",
    "E_NN = 1   # nonlandslide-nonlandslide (negatives)\n",
    "E_LN = 2   # landslide-nonlandslide (bidirectional contrast edges)\n",
    "E_UL = 3   # unlabeled pseudo-positive <-> train positive anchors (dynamic)\n",
    "E_UN = 4   # unlabeled pseudo-negative <-> train negative anchors (dynamic)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Optional synthetic dataset (for runnable demo without real data)\n",
    "# ============================================================\n",
    "def generate_synthetic_df(\n",
    "    n_total: int = 2000,\n",
    "    n_pos: int = 300,\n",
    "    n_neg: int = 300,\n",
    "    seed: int = 2\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a synthetic dataset for sanity checks.\n",
    "    Columns:\n",
    "      - Id: int\n",
    "      - label: 1/0/-1 (1=positive, 0=negative, -1=unlabeled)\n",
    "      - ls_type: 1..5 for positives, 0 otherwise\n",
    "      - FEATURE_COLS: float in [0,1]\n",
    "    \"\"\"\n",
    "    assert n_pos + n_neg <= n_total\n",
    "    set_seed(seed)\n",
    "\n",
    "    n_unl = n_total - n_pos - n_neg\n",
    "    pos_types = np.random.choice([1, 2, 3, 4, 5], size=n_pos, replace=True)\n",
    "\n",
    "    base_neg = np.array([0.45] * len(FEATURE_COLS), dtype=float)\n",
    "    base_pos = np.array([0.55] * len(FEATURE_COLS), dtype=float)\n",
    "\n",
    "    type_shift = {t: np.zeros(len(FEATURE_COLS), dtype=float) for t in [1, 2, 3, 4, 5]}\n",
    "    col_to_i = {c: i for i, c in enumerate(FEATURE_COLS)}\n",
    "\n",
    "    for t, cols in TYPE_FEATURES.items():\n",
    "        for c in cols:\n",
    "            type_shift[t][col_to_i[c]] += 0.18\n",
    "\n",
    "    def sample_block(n, mean_vec, noise=0.10):\n",
    "        X = mean_vec + noise * np.random.randn(n, len(FEATURE_COLS))\n",
    "        return np.clip(X, 0.0, 1.0)\n",
    "\n",
    "    X_pos = np.zeros((n_pos, len(FEATURE_COLS)), dtype=float)\n",
    "    for t in [1, 2, 3, 4, 5]:\n",
    "        idx = np.where(pos_types == t)[0]\n",
    "        if idx.size:\n",
    "            X_pos[idx] = sample_block(idx.size, base_pos + type_shift[t], noise=0.08)\n",
    "\n",
    "    X_neg = sample_block(n_neg, base_neg, noise=0.10)\n",
    "\n",
    "    unl_mix = np.random.rand(n_unl)\n",
    "    X_unl = np.zeros((n_unl, len(FEATURE_COLS)), dtype=float)\n",
    "    for i in range(n_unl):\n",
    "        if unl_mix[i] < 0.45:\n",
    "            X_unl[i] = sample_block(1, base_neg, noise=0.12)[0]\n",
    "        else:\n",
    "            t = np.random.choice([1, 2, 3, 4, 5])\n",
    "            X_unl[i] = sample_block(1, base_pos + 0.7 * type_shift[t], noise=0.10)[0]\n",
    "\n",
    "    Id = np.arange(1, n_total + 1, dtype=int)\n",
    "    label = np.array([1] * n_pos + [0] * n_neg + [-1] * n_unl, dtype=int)\n",
    "    ls_type = np.array(list(pos_types) + [0] * n_neg + [0] * n_unl, dtype=int)\n",
    "\n",
    "    X = np.vstack([X_pos, X_neg, X_unl])\n",
    "    df = pd.DataFrame(X, columns=FEATURE_COLS)\n",
    "    df.insert(0, \"ls_type\", ls_type)\n",
    "    df.insert(0, \"label\", label)\n",
    "    df.insert(0, \"Id\", Id)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) Graph utilities: cosine top-k edges\n",
    "# ============================================================\n",
    "def topk_cosine_edges(global_indices: np.ndarray, X: np.ndarray, k: int):\n",
    "    \"\"\"\n",
    "    Build within-set top-k cosine similarity edges.\n",
    "    global_indices are node indices in the full graph (aligned with df row indices).\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    if n == 0:\n",
    "        return []\n",
    "    Xn = normalize(X, axis=1)\n",
    "    S = Xn @ Xn.T  # (n, n)\n",
    "\n",
    "    edges = []\n",
    "    for i in range(n):\n",
    "        order = np.argsort(-S[i])\n",
    "        top = order[: min(k, n)]  # may include self-loop\n",
    "        src = int(global_indices[i])\n",
    "        for j in top:\n",
    "            edges.append((src, int(global_indices[j])))\n",
    "    return edges\n",
    "\n",
    "\n",
    "def topk_cosine_cross_edges(src_global, X_src, dst_global, X_dst, k: int, bidirectional: bool = True):\n",
    "    \"\"\"\n",
    "    Build cross-set edges using top-k cosine similarity from src -> dst.\n",
    "    If bidirectional=True, add reverse edges as well.\n",
    "    \"\"\"\n",
    "    if X_src.shape[0] == 0 or X_dst.shape[0] == 0:\n",
    "        return []\n",
    "\n",
    "    A = normalize(X_src, axis=1)\n",
    "    B = normalize(X_dst, axis=1)\n",
    "    S = A @ B.T  # (nsrc, ndst)\n",
    "\n",
    "    edges = []\n",
    "    for i in range(S.shape[0]):\n",
    "        order = np.argsort(-S[i])\n",
    "        top = order[: min(k, S.shape[1])]\n",
    "        s = int(src_global[i])\n",
    "        for j in top:\n",
    "            t = int(dst_global[j])\n",
    "            edges.append((s, t))\n",
    "            if bidirectional:\n",
    "                edges.append((t, s))\n",
    "    return edges\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) Build static graph (E_LL / E_NN / E_LN)\n",
    "# ============================================================\n",
    "def build_static_graph(df: pd.DataFrame, eta_intra: int = 7, eta_nn: int = 7, eta_ln: int = 7):\n",
    "    \"\"\"\n",
    "    Static graph uses only labeled nodes (label in {1,0}):\n",
    "      - E_LL: within-type edges among positive clusters (ls_type in 1..5)\n",
    "      - E_NN: within-class edges among negatives\n",
    "      - E_LN: bidirectional cross-class edges (positive <-> negative)\n",
    "\n",
    "    Note:\n",
    "      Unlabeled nodes (label=-1) are not connected by static edges here.\n",
    "      They receive messages only when dynamic edges (E_UL/E_UN) are added during training.\n",
    "    \"\"\"\n",
    "    node_ids = df[\"Id\"].values\n",
    "    pos_idx = df.index[df[\"label\"] == 1].to_numpy()\n",
    "    neg_idx = df.index[df[\"label\"] == 0].to_numpy()\n",
    "\n",
    "    edge_list = []\n",
    "    edge_type = []\n",
    "\n",
    "    # E_LL: within-type similarity among positive nodes\n",
    "    for t in [1, 2, 3, 4, 5]:\n",
    "        sub = df[(df[\"label\"] == 1) & (df[\"ls_type\"] == t)]\n",
    "        if sub.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        cols = TYPE_FEATURES.get(t, FEATURE_COLS)\n",
    "        if not all(c in sub.columns for c in cols):\n",
    "            cols = FEATURE_COLS\n",
    "\n",
    "        gidx = sub.index.to_numpy()\n",
    "        X = sub[cols].values.astype(float)\n",
    "        edges = topk_cosine_edges(gidx, X, eta_intra)\n",
    "        edge_list.extend(edges)\n",
    "        edge_type.extend([E_LL] * len(edges))\n",
    "\n",
    "    # E_NN: within-class similarity among negative nodes\n",
    "    if neg_idx.size > 0:\n",
    "        Xn = df.loc[neg_idx, FEATURE_COLS].values.astype(float)\n",
    "        edges_nn = topk_cosine_edges(neg_idx, Xn, eta_nn)\n",
    "        edge_list.extend(edges_nn)\n",
    "        edge_type.extend([E_NN] * len(edges_nn))\n",
    "\n",
    "    # E_LN: cross-class edges (bidirectional)\n",
    "    if pos_idx.size > 0 and neg_idx.size > 0:\n",
    "        Xp = df.loc[pos_idx, FEATURE_COLS].values.astype(float)\n",
    "        Xn = df.loc[neg_idx, FEATURE_COLS].values.astype(float)\n",
    "        edges_ln = topk_cosine_cross_edges(pos_idx, Xp, neg_idx, Xn, eta_ln, bidirectional=True)\n",
    "        edge_list.extend(edges_ln)\n",
    "        edge_type.extend([E_LN] * len(edges_ln))\n",
    "\n",
    "    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "    edge_type = torch.tensor(edge_type, dtype=torch.long)\n",
    "    return node_ids, edge_index, edge_type\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) Train/Val/Test split (labeled nodes only)\n",
    "# ============================================================\n",
    "def make_splits(\n",
    "    y: torch.Tensor,\n",
    "    seed: int = 2,\n",
    "    train_ratio: float = 0.7,\n",
    "    val_ratio: float = 0.15,\n",
    "    test_ratio: float = 0.15\n",
    "):\n",
    "    \"\"\"\n",
    "    Split labeled nodes only (y != -1):\n",
    "      1) labeled -> train/test\n",
    "      2) train -> train/val\n",
    "    \"\"\"\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6\n",
    "\n",
    "    labeled = (y != -1).nonzero(as_tuple=True)[0].cpu().numpy()\n",
    "    y_lab = y[labeled].cpu().numpy()\n",
    "\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        labeled,\n",
    "        test_size=test_ratio,\n",
    "        random_state=seed,\n",
    "        stratify=y_lab\n",
    "    )\n",
    "\n",
    "    y_train = y[train_idx].cpu().numpy()\n",
    "    val_frac_in_train = val_ratio / (train_ratio + val_ratio)\n",
    "\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        train_idx,\n",
    "        test_size=val_frac_in_train,\n",
    "        random_state=seed,\n",
    "        stratify=y_train\n",
    "    )\n",
    "\n",
    "    n = y.shape[0]\n",
    "    train_mask = torch.zeros(n, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(n, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(n, dtype=torch.bool)\n",
    "\n",
    "    train_mask[train_idx] = True\n",
    "    val_mask[val_idx] = True\n",
    "    test_mask[test_idx] = True\n",
    "    return train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) Model: node-att + edge-att (sum) -> MLP\n",
    "# ============================================================\n",
    "class NodeEdgeFusionGAT(nn.Module):\n",
    "    \"\"\"\n",
    "    Two parallel branches:\n",
    "      - node_att: node-level attention (no edge attributes)\n",
    "      - edge_att: edge-semantic attention (edge attributes from edge types)\n",
    "    Fusion: element-wise sum, then MLP classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim, num_edge_types=5, heads=5, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.edge_type_emb = nn.Embedding(num_edge_types, in_dim)\n",
    "\n",
    "        self.node_att = GATConv(\n",
    "            in_channels=in_dim, out_channels=hidden_dim,\n",
    "            heads=heads, concat=True, dropout=dropout\n",
    "        )\n",
    "        self.edge_att = GATConv(\n",
    "            in_channels=in_dim, out_channels=hidden_dim,\n",
    "            heads=heads, concat=True, dropout=dropout, edge_dim=in_dim\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * heads, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_type):\n",
    "        edge_attr = self.edge_type_emb(edge_type)\n",
    "        h_node = F.elu(self.node_att(x, edge_index))\n",
    "        h_edge = F.elu(self.edge_att(x, edge_index, edge_attr))\n",
    "        h = h_node + h_edge\n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        return torch.sigmoid(self.classifier(h).squeeze())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8) Dynamic edge updater (recomputed, non-cumulative)\n",
    "# ============================================================\n",
    "class DynamicEdgeUpdater:\n",
    "    \"\"\"\n",
    "    Dynamic edges are rebuilt every update (non-cumulative):\n",
    "      - Anchors come from train_mask only (prevents val/test leakage).\n",
    "      - Unlabeled nodes are selected by high-confidence predictions.\n",
    "      - Pseudo labels are stored in data.pseudo_y but NOT used in the supervised loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, b=7, c=7, confidence_threshold=0.8):\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "        self.threshold = confidence_threshold\n",
    "\n",
    "    def update_edges(self, data: Data, model: nn.Module):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            probs = model(data.x, data.edge_index, data.edge_type)\n",
    "\n",
    "        unlabeled = (data.y == -1)\n",
    "        pseudo_pos = (probs > self.threshold) & unlabeled\n",
    "        pseudo_neg = (probs < (1 - self.threshold)) & unlabeled\n",
    "\n",
    "        pos_anchor = torch.where((data.y == 1) & data.train_mask)[0]\n",
    "        neg_anchor = torch.where((data.y == 0) & data.train_mask)[0]\n",
    "\n",
    "        if pos_anchor.numel() == 0 or neg_anchor.numel() == 0:\n",
    "            data.edge_index = data.edge_index_static\n",
    "            data.edge_type = data.edge_type_static\n",
    "            return data\n",
    "\n",
    "        new_edges = []\n",
    "        new_types = []\n",
    "\n",
    "        # Build bidirectional edges for stable message passing\n",
    "        for src in torch.where(pseudo_pos)[0]:\n",
    "            sim = F.cosine_similarity(data.x[pos_anchor], data.x[src].unsqueeze(0), dim=-1)\n",
    "            k = min(self.b, sim.numel())\n",
    "            topk = torch.topk(sim, k=k).indices\n",
    "            for dst in pos_anchor[topk]:\n",
    "                s = int(src.item()); d = int(dst.item())\n",
    "                new_edges.append([s, d]); new_types.append(E_UL)\n",
    "                new_edges.append([d, s]); new_types.append(E_UL)\n",
    "\n",
    "        for src in torch.where(pseudo_neg)[0]:\n",
    "            sim = F.cosine_similarity(data.x[neg_anchor], data.x[src].unsqueeze(0), dim=-1)\n",
    "            k = min(self.c, sim.numel())\n",
    "            topk = torch.topk(sim, k=k).indices\n",
    "            for dst in neg_anchor[topk]:\n",
    "                s = int(src.item()); d = int(dst.item())\n",
    "                new_edges.append([s, d]); new_types.append(E_UN)\n",
    "                new_edges.append([d, s]); new_types.append(E_UN)\n",
    "\n",
    "        # Rebuild dynamic graph: static edges + current dynamic edges\n",
    "        if len(new_edges) > 0:\n",
    "            new_edge_index = torch.tensor(new_edges, device=data.edge_index.device, dtype=torch.long).t()\n",
    "            new_edge_type = torch.tensor(new_types, device=data.edge_type.device, dtype=data.edge_type.dtype)\n",
    "            data.edge_index = torch.cat([data.edge_index_static, new_edge_index], dim=1)\n",
    "            data.edge_type = torch.cat([data.edge_type_static, new_edge_type], dim=0)\n",
    "        else:\n",
    "            data.edge_index = data.edge_index_static\n",
    "            data.edge_type = data.edge_type_static\n",
    "\n",
    "        pseudo_y = torch.full_like(data.y, -1)\n",
    "        pseudo_y[pseudo_pos] = 1\n",
    "        pseudo_y[pseudo_neg] = 0\n",
    "        data.pseudo_y = pseudo_y\n",
    "        return data\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9) Evaluation helpers\n",
    "# ============================================================\n",
    "def eval_split(model, data, mask, criterion):\n",
    "    \"\"\"Compute loss/accuracy/AUC on a specific split mask.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prob = model(data.x, data.edge_index, data.edge_type)\n",
    "        y_true = data.y[mask].float()\n",
    "        y_prob = prob[mask]\n",
    "\n",
    "        loss = criterion(y_prob, y_true).item()\n",
    "\n",
    "        y_pred = (y_prob > 0.5).long().cpu().numpy()\n",
    "        y_true_i = data.y[mask].long().cpu().numpy()\n",
    "        acc = accuracy_score(y_true_i, y_pred)\n",
    "\n",
    "        auc = float(\"nan\")\n",
    "        if len(np.unique(y_true_i)) == 2:\n",
    "            auc = roc_auc_score(y_true_i, y_prob.detach().cpu().numpy())\n",
    "\n",
    "    return loss, acc, auc\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 10) Jupyter entry (read real data if provided, else use synthetic)\n",
    "# ============================================================\n",
    "set_seed(2)\n",
    "\n",
    "# Set this to your processed file path (xlsx/csv) if available.\n",
    "# Required columns: Id, label, ls_type, and FEATURE_COLS.\n",
    "DATA_PATH = None\n",
    "\n",
    "if DATA_PATH is not None and os.path.exists(DATA_PATH):\n",
    "    if DATA_PATH.lower().endswith(\".csv\"):\n",
    "        df = pd.read_csv(DATA_PATH)\n",
    "    else:\n",
    "        df = pd.read_excel(DATA_PATH)\n",
    "else:\n",
    "    df = generate_synthetic_df(n_total=2000, n_pos=300, n_neg=300, seed=2)\n",
    "\n",
    "required_cols = [\"Id\", \"label\", \"ls_type\"] + FEATURE_COLS\n",
    "for col in required_cols:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Missing required column: {col}\")\n",
    "\n",
    "node_ids, edge_index, edge_type = build_static_graph(df, eta_intra=7, eta_nn=7, eta_ln=7)\n",
    "\n",
    "x = torch.tensor(df[FEATURE_COLS].values.astype(np.float32))\n",
    "y = torch.tensor(df[\"label\"].values.astype(np.int64))\n",
    "\n",
    "train_mask, val_mask, test_mask = make_splits(y, seed=2, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15)\n",
    "\n",
    "data = Data(\n",
    "    x=x,\n",
    "    edge_index=edge_index,\n",
    "    edge_type=edge_type,\n",
    "    y=y,\n",
    "    train_mask=train_mask,\n",
    "    val_mask=val_mask,\n",
    "    test_mask=test_mask\n",
    ")\n",
    "\n",
    "# Edge-type sanity check\n",
    "print(\"edge_type min/max:\", int(data.edge_type.min()), int(data.edge_type.max()))\n",
    "assert int(data.edge_type.min()) >= 0\n",
    "assert int(data.edge_type.max()) <= 4, \"edge_type must be within 0..4.\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data = data.to(device)\n",
    "\n",
    "model = NodeEdgeFusionGAT(\n",
    "    in_dim=len(FEATURE_COLS),\n",
    "    hidden_dim=128,\n",
    "    num_edge_types=5,\n",
    "    heads=5,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-8)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Important: keep static backups AFTER moving data to device\n",
    "data.edge_index_static = data.edge_index.clone()\n",
    "data.edge_type_static = data.edge_type.clone()\n",
    "\n",
    "updater = DynamicEdgeUpdater(b=7, c=7, confidence_threshold=0.8)\n",
    "\n",
    "epochs = 60\n",
    "update_every = 5\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Dynamic update: recompute (non-cumulative)\n",
    "    if update_every > 0 and epoch % update_every == 0:\n",
    "        data = updater.update_edges(data, model)\n",
    "\n",
    "    prob = model(data.x, data.edge_index, data.edge_type)\n",
    "\n",
    "    # Supervised loss uses train_mask only (pseudo labels are NOT used in loss)\n",
    "    loss = criterion(prob[data.train_mask], data.y[data.train_mask].float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        tr = eval_split(model, data, data.train_mask, criterion)\n",
    "        va = eval_split(model, data, data.val_mask, criterion)\n",
    "        te = eval_split(model, data, data.test_mask, criterion)\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d} | \"\n",
    "            f\"Train loss/acc/auc: {tr[0]:.4f}/{tr[1]:.4f}/{tr[2]:.4f} | \"\n",
    "            f\"Val loss/acc/auc: {va[0]:.4f}/{va[1]:.4f}/{va[2]:.4f} | \"\n",
    "            f\"Test loss/acc/auc: {te[0]:.4f}/{te[1]:.4f}/{te[2]:.4f}\"\n",
    "        )\n",
    "\n",
    "tr = eval_split(model, data, data.train_mask, criterion)\n",
    "va = eval_split(model, data, data.val_mask, criterion)\n",
    "te = eval_split(model, data, data.test_mask, criterion)\n",
    "\n",
    "print(\"\\nFinal:\")\n",
    "print(f\"Train: loss={tr[0]:.4f}, acc={tr[1]:.4f}, auc={tr[2]:.4f}\")\n",
    "print(f\"Val  : loss={va[0]:.4f}, acc={va[1]:.4f}, auc={va[2]:.4f}\")\n",
    "print(f\"Test : loss={te[0]:.4f}, acc={te[1]:.4f}, auc={te[2]:.4f}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    prob_all = model(data.x, data.edge_index, data.edge_type).detach().cpu().numpy()\n",
    "\n",
    "out_df = pd.DataFrame({\"Id\": node_ids, \"prob\": prob_all})\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "out_df.to_csv(\"outputs/predictions_all_nodes.csv\", index=False)\n",
    "print(\"Saved: outputs/predictions_all_nodes.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5b130c-7827-4edf-9626-c88e3a30aa9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
